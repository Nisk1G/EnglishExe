@ARTICLE{Transformer,
       author = {{Vaswani}, Ashish and {Shazeer}, Noam and {Parmar}, Niki and {Uszkoreit}, Jakob and {Jones}, Llion and {Gomez}, Aidan N. and {Kaiser}, Lukasz and {Polosukhin}, Illia},
        title = "{Attention Is All You Need}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2017,
        month = jun,
          eid = {arXiv:1706.03762},
        pages = {arXiv:1706.03762},
archivePrefix = {arXiv},
       eprint = {1706.03762},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170603762V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
},
@misc{BLEU,
        author  = {TOIN},
        title   = {機械翻訳の評価に最もよく用いられる「BLEUスコア」とは},
        note    = {\url{https://to-in.com/blog/102282}},
        year    = {2020.03.02}
},
@misc{ensemble,
        author = {codExa},
        title  = {機械学習上級者は皆使ってる？！アンサンブル学習の仕組みと3つの種類について解説します},
        note   = {\url{https://www.codexa.net/what-is-ensemble-learning/}},
        year   = {2018.06.21}
}
@misc{LSTM,
        author = {KojiOhki},
        title  = {LSTMネットワークの概要},
        note   = {\url{https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca}},
        year   = {2017.12.11},
}

@misc{GRU,
        author = {DeepAge},
        title  = {RNN：時系列データを扱うRecurrent Neural Networksとは},
        note   = {\url{https://deepage.net/deep_learning/2017/05/23/recurrent-neural-networks.html}},
        year   = {2017.05.23}
}
@misc{自己回帰モデル,
        author = {biostatistics},
        title  = {自己回帰（AR）モデル},
        note   = {\url{https://stats.biopapyrus.jp/time-series/ar-model.html}},
        year   = {}
}
@misc{残差接続,
        author = {DeepAge},
        title  = {Residual Network(ResNet)の理解とチューニングのベストプラクティス},
        note   = {\url{https://deepage.net/deep_learning/2016/11/30/resnet.html}},
        year   = {2016.11.30}
}
@misc{Multihead,
        author = {Masaki Hayashi,CVMLエキスパートガイド},
        title  = {マルチヘッドアテンション (Multi-head Attention) [Transformer]},
        note   = {\url{https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/multi-head-attention/}},
        year   = {2022.07.02}
}
@misc{Dropout,
        author = {Shuhei Kishi},
        title  = {【ニューラルネットワーク】Dropout(ドロップアウト)についてまとめる},
        note   = {\url{https://qiita.com/shu_marubo/items/70b20c3a6c172aaeb8de}},
        year   = {2017.07.18}
}
@misc{LabelSmoothing,
        author = {T-STAR},
        title  = {Online Label Smoothingの実装と評価},
        note   = {\url{https://qiita.com/T-STAR/items/a3bdcd1ae00150fe1402}},
        year   = {2021.04.01}
}
@misc{FLOPS,
        author = {FUJITSU},
        title  = {お答えします スパコンQ\&A},
        note   = {\url{https://www.fujitsu.com/jp/about/businesspolicy/tech/k/qa/k02.html}},
        year   = {}
}
@ARTICLE{1,
       author = {{Lei Ba}, Jimmy and {Kiros}, Jamie Ryan and {Hinton}, Geoffrey E.},
        title = "{Layer Normalization}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2016,
        month = jul,
          eid = {arXiv:1607.06450},
        pages = {arXiv:1607.06450},
archivePrefix = {arXiv},
       eprint = {1607.06450},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160706450L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2,
       author = {{Bahdanau}, Dzmitry and {Cho}, Kyunghyun and {Bengio}, Yoshua},
        title = "{Neural Machine Translation by Jointly Learning to Align and Translate}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2014,
        month = sep,
          eid = {arXiv:1409.0473},
        pages = {arXiv:1409.0473},
archivePrefix = {arXiv},
       eprint = {1409.0473},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1409.0473B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{3,
       author = {{Britz}, Denny and {Goldie}, Anna and {Luong}, Minh-Thang and {Le}, Quoc},
        title = "{Massive Exploration of Neural Machine Translation Architectures}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.03906},
        pages = {arXiv:1703.03906},
archivePrefix = {arXiv},
       eprint = {1703.03906},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303906B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{4,
       author = {{Cheng}, Jianpeng and {Dong}, Li and {Lapata}, Mirella},
        title = "{Long Short-Term Memory-Networks for Machine Reading}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
         year = 2016,
        month = jan,
          eid = {arXiv:1601.06733},
        pages = {arXiv:1601.06733},
archivePrefix = {arXiv},
       eprint = {1601.06733},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160106733C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{5,
       author = {{Cho}, Kyunghyun and {van Merrienboer}, Bart and {Gulcehre}, Caglar and {Bahdanau}, Dzmitry and {Bougares}, Fethi and {Schwenk}, Holger and {Bengio}, Yoshua},
        title = "{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2014,
        month = jun,
          eid = {arXiv:1406.1078},
        pages = {arXiv:1406.1078},
archivePrefix = {arXiv},
       eprint = {1406.1078},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1406.1078C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{6,
       author = {{Chollet}, Fran{\c{c}}ois},
        title = "{Xception: Deep Learning with Depthwise Separable Convolutions}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2016,
        month = oct,
          eid = {arXiv:1610.02357},
        pages = {arXiv:1610.02357},
archivePrefix = {arXiv},
       eprint = {1610.02357},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161002357C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{7,
       author = {{Chung}, Junyoung and {Gulcehre}, Caglar and {Cho}, KyungHyun and {Bengio}, Yoshua},
        title = "{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
         year = 2014,
        month = dec,
          eid = {arXiv:1412.3555},
        pages = {arXiv:1412.3555},
archivePrefix = {arXiv},
       eprint = {1412.3555},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3555C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{8,
       author = {{Gehring}, Jonas and {Auli}, Michael and {Grangier}, David and {Yarats}, Denis and {Dauphin}, Yann N.},
        title = "{Convolutional Sequence to Sequence Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2017,
        month = may,
          eid = {arXiv:1705.03122},
        pages = {arXiv:1705.03122},
archivePrefix = {arXiv},
       eprint = {1705.03122},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170503122G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{9,
       author = {{Graves}, Alex},
        title = "{Generating Sequences With Recurrent Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
         year = 2013,
        month = aug,
          eid = {arXiv:1308.0850},
        pages = {arXiv:1308.0850},
archivePrefix = {arXiv},
       eprint = {1308.0850},
 primaryClass = {cs.NE},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1308.0850G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{10,
       author = {{He}, Kaiming and {Zhang}, Xiangyu and {Ren}, Shaoqing and {Sun}, Jian},
        title = "{Deep Residual Learning for Image Recognition}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = dec,
          eid = {arXiv:1512.03385},
        pages = {arXiv:1512.03385},
archivePrefix = {arXiv},
       eprint = {1512.03385},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151203385H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{11,
  title={Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  author={Sepp Hochreiter and Yoshua Bengio},
  year={2001}
}
@Article{12,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}
@ARTICLE{13,
       author = {{Jozefowicz}, Rafal and {Vinyals}, Oriol and {Schuster}, Mike and {Shazeer}, Noam and {Wu}, Yonghui},
        title = "{Exploring the Limits of Language Modeling}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2016,
        month = feb,
          eid = {arXiv:1602.02410},
        pages = {arXiv:1602.02410},
archivePrefix = {arXiv},
       eprint = {1602.02410},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160202410J},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{14,
       author = {{Kaiser}, {\L}ukasz and {Sutskever}, Ilya},
        title = "{Neural GPUs Learn Algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = 2015,
        month = nov,
          eid = {arXiv:1511.08228},
        pages = {arXiv:1511.08228},
archivePrefix = {arXiv},
       eprint = {1511.08228},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151108228K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{15,
       author = {{Kalchbrenner}, Nal and {Espeholt}, Lasse and {Simonyan}, Karen and {van den Oord}, Aaron and {Graves}, Alex and {Kavukcuoglu}, Koray},
        title = "{Neural Machine Translation in Linear Time}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2016,
        month = oct,
          eid = {arXiv:1610.10099},
        pages = {arXiv:1610.10099},
archivePrefix = {arXiv},
       eprint = {1610.10099},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161010099K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{16,
       author = {{Kim}, Yoon and {Denton}, Carl and {Hoang}, Luong and {Rush}, Alexander M.},
        title = "{Structured Attention Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = 2017,
        month = feb,
          eid = {arXiv:1702.00887},
        pages = {arXiv:1702.00887},
archivePrefix = {arXiv},
       eprint = {1702.00887},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170200887K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{17,
       author = {{Kingma}, Diederik P. and {Ba}, Jimmy},
        title = "{Adam: A Method for Stochastic Optimization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2014,
        month = dec,
          eid = {arXiv:1412.6980},
        pages = {arXiv:1412.6980},
archivePrefix = {arXiv},
       eprint = {1412.6980},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{18,
       author = {{Kuchaiev}, Oleksii and {Ginsburg}, Boris},
        title = "{Factorization tricks for LSTM networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.10722},
        pages = {arXiv:1703.10722},
archivePrefix = {arXiv},
       eprint = {1703.10722},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170310722K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{19,
       author = {{Lin}, Zhouhan and {Feng}, Minwei and {Nogueira dos Santos}, Cicero and {Yu}, Mo and {Xiang}, Bing and {Zhou}, Bowen and {Bengio}, Yoshua},
        title = "{A Structured Self-attentive Sentence Embedding}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.03130},
        pages = {arXiv:1703.03130},
archivePrefix = {arXiv},
       eprint = {1703.03130},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170303130L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{20,
       author = {{Kaiser}, {\L}ukasz and {Bengio}, Samy},
        title = "{Can Active Memory Replace Attention?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
         year = 2016,
        month = oct,
          eid = {arXiv:1610.08613},
        pages = {arXiv:1610.08613},
archivePrefix = {arXiv},
       eprint = {1610.08613},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161008613K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{21,
       author = {{Luong}, Minh-Thang and {Pham}, Hieu and {Manning}, Christopher D.},
        title = "{Effective Approaches to Attention-based Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2015,
        month = aug,
          eid = {arXiv:1508.04025},
        pages = {arXiv:1508.04025},
archivePrefix = {arXiv},
       eprint = {1508.04025},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150804025L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{22,
       author = {{Parikh}, Ankur P. and {T{\"a}ckstr{\"o}m}, Oscar and {Das}, Dipanjan and {Uszkoreit}, Jakob},
        title = "{A Decomposable Attention Model for Natural Language Inference}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2016,
        month = jun,
          eid = {arXiv:1606.01933},
        pages = {arXiv:1606.01933},
archivePrefix = {arXiv},
       eprint = {1606.01933},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160601933P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{23,
       author = {{Paulus}, Romain and {Xiong}, Caiming and {Socher}, Richard},
        title = "{A Deep Reinforced Model for Abstractive Summarization}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2017,
        month = may,
          eid = {arXiv:1705.04304},
        pages = {arXiv:1705.04304},
archivePrefix = {arXiv},
       eprint = {1705.04304},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170504304P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{24,
       author = {{Press}, Ofir and {Wolf}, Lior},
        title = "{Using the Output Embedding to Improve Language Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2016,
        month = aug,
          eid = {arXiv:1608.05859},
        pages = {arXiv:1608.05859},
archivePrefix = {arXiv},
       eprint = {1608.05859},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160805859P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{25,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
@ARTICLE{26,
       author = {{Shazeer}, Noam and {Mirhoseini}, Azalia and {Maziarz}, Krzysztof and {Davis}, Andy and {Le}, Quoc and {Hinton}, Geoffrey and {Dean}, Jeff},
        title = "{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2017,
        month = jan,
          eid = {arXiv:1701.06538},
        pages = {arXiv:1701.06538},
archivePrefix = {arXiv},
       eprint = {1701.06538},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170106538S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{27,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@inproceedings{28,
 author = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {2440-2448},
 publisher = {Curran Associates, Inc.},
 title = {End-To-End Memory Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
 volume = {28},
 year = {2015}
}
@inproceedings{29,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}
@ARTICLE{30,
       author = {{Szegedy}, Christian and {Vanhoucke}, Vincent and {Ioffe}, Sergey and {Shlens}, Jonathon and {Wojna}, Zbigniew},
        title = "{Rethinking the Inception Architecture for Computer Vision}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2015,
        month = dec,
          eid = {arXiv:1512.00567},
        pages = {arXiv:1512.00567},
archivePrefix = {arXiv},
       eprint = {1512.00567},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151200567S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{31,
       author = {{Wu}, Yonghui and {Schuster}, Mike and {Chen}, Zhifeng and {Le}, Quoc V. and {Norouzi}, Mohammad and {Macherey}, Wolfgang and {Krikun}, Maxim and {Cao}, Yuan and {Gao}, Qin and {Macherey}, Klaus and {Klingner}, Jeff and {Shah}, Apurva and {Johnson}, Melvin and {Liu}, Xiaobing and {Kaiser}, {\L}ukasz and {Gouws}, Stephan and {Kato}, Yoshikiyo and {Kudo}, Taku and {Kazawa}, Hideto and {Stevens}, Keith and {Kurian}, George and {Patil}, Nishant and {Wang}, Wei and {Young}, Cliff and {Smith}, Jason and {Riesa}, Jason and {Rudnick}, Alex and {Vinyals}, Oriol and {Corrado}, Greg and {Hughes}, Macduff and {Dean}, Jeffrey},
        title = "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2016,
        month = sep,
          eid = {arXiv:1609.08144},
        pages = {arXiv:1609.08144},
archivePrefix = {arXiv},
       eprint = {1609.08144},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160908144W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{32,
       author = {{Zhou}, Jie and {Cao}, Ying and {Wang}, Xuguang and {Li}, Peng and {Xu}, Wei},
        title = "{Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2016,
        month = jun,
          eid = {arXiv:1606.04199},
        pages = {arXiv:1606.04199},
archivePrefix = {arXiv},
       eprint = {1606.04199},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160604199Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@misc{膨張畳込み,
        author  = {Masaki Hayashi},
        title   = {膨張畳み込み(Dilated Convolution)},
        note    = {\url{https://cvml-expertguide.net/terms/dl/layers/convolution-layer/dilated-convolution/}},
        year    = {2022.05.15}
}













